%% LyX 2.2.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{url}
\usepackage{amstext}
\usepackage{babel}
\begin{document}

\title{The MiniWeather Mini App}

\author{Matt Norman, Oak Ridge National Laboratory}

\date{~}

\maketitle
\tableofcontents{}

\section{Introduction}

The miniWeather code mimics the basic dynamics seen in atmospheric
weather and climate. The dynamics themselves are dry compressible,
stratified, non-hydrostatic flows dominated by buoyant forces that
are relatively small perturbations on a hydrostatic background state.
The equations in this code themselves form the backbone of pretty
much all fluid dynamics codes, and this particular flavor forms the
base of all weather and climate modeling.

With about 500 total lines of code (and only about 200 lines that
you care about), it serves as an approachable place to learn parallelization
and porting using MPI + X, where X is OpenMP, OpenACC, CUDA, or potentially
other approaches to CPU and accelerated parallelization. The code
uses periodic boundary conditions in the $x$-direction and solid
wall boundary conditions in the $z$-direction. 

\section{Compiling and Running the Code}

\subsection{Required Libraries}

To run everything in this code, you need MPI, parallel-netcdf, ncview,
and an OpenACC compiler (preferably PGI 17+) to be installed. The
serial code still uses MPI because I figured it was best to start
with parallel I/O in place so that it's ready to go when the MPI implementation
is done. 

You can download parallel-netcdf from:\\
\url{https://trac.mcs.anl.gov/projects/parallel-netcdf}\\
Also, you can download a free version of PGI Community Edition (which
has OpenACC implemented) from:\\
\url{https://www.pgroup.com/products/community.htm}\\
You'll probably also want to have ``ncview'' installed so you can
easily view the NetCDF files. Ncview also makes it easy to see a movie
of the solution as it evolves in time:\\
\url{http://meteora.ucsd.edu/~pierce/ncview_home_page.html}

\subsection{Directories and Compiling}

There are three main directories in the mini app: (1) a Fortran source
directory; (2) a C source directory; and (3) a documentation directory,
which you presumably know about because it contains this text you're
reading now. The C code is technically C++ code but only because I
wanted to use the ampersand pass by reference notation because C's
endless asterisks make me want to die. I kept the code from C++ proper
because it makes OpenACC and OpenMP 4.5 easier to use, though multidimensional
array indexing in C is cumbersome, to say the least.

To compile the code, first edit the Makefile and change the flags
to point to your parallel-netcdf installation as well as change the
flags based on which compiler you are using. There are four versions
of the code: serial, mpi, mpi+openmp, and mpi+openacc. The filenames
make it clear which file is associated with which programming paradigm.
To make all of these at once, simply type ``make''. To make them
individually, you can type ``make serial'', ``make mpi'', ``make
openmp'', and ``make openacc''.

\subsection{Altering the Code's Configurations}

There are four aspects of the configuration you can edit easily, and
they are clearly labeled in the code as ``USER-CONFIGURABLE PARAMETERS''.
These include: (1) the number of cells to use {[}``nx\_glob'' and
``nz\_glob''{]}; (2) the amount of time to simulation {[}``sim\_time''{]};
(3) the frequency to output data to file {[}``output\_freq''{]};
and (4) the initial data to use {[}``data\_spec\_int''{]}. 

\subsection{Running the Code}

To run the code, simply call ``mpirun -n {[}\# ranks{]} ./mini\_weather\_{[}version{]}''.
Since parameters are set in the code itself, you don't need to pass
any parameters. Some machines use different tools instead of mpirun
(e.g., OLCF's Titan uses ``aprun'').

\subsection{Viewing the Output}

The file I/O is done in the netCDF format: (\url{https://www.unidata.ucar.edu/software/netcdf/}).
To me, the easiest way to view the data is to use a tool called ``ncview''
(\url{http://meteora.ucsd.edu/~pierce/ncview_home_page.html}). To
use it, you can simply type ``ncview output.nc'', making sure you
have X-forwarding enabled in your ssh session. Further, you can call
``ncview -frames output.nc'', and it will dump out all of your frames
in the native resolution you're viewing the data in, and you you can
render a movie with tools like ffmpeg. 

\section{Parallelization}

This code was designed to parallelize with MPI first and then OpenMP
or OpenACC next, but you can always parallelize with OpenMP or OpenACC
without MPI if you want. But it is rewarding to be able to run it
on multiple nodes at higher resolution for more and sharper eddies
in the dynamics. 

As you port the code, you'll want to change relatively little code
at a time, re-compile, re-run, and look at the output to see that
you're still getting the right answer. There are advantages to using
a visual tool to check the answer, as it can sometimes give you clues
as to why you're not getting the right answer. 

Note that you only need to make changes code within the first 450
source lines. Everything below that is initialization and I/O code
that doesn't need to be parallelized (unless you want to). There's
a note in the serial code that tells you at what point the rest of
the code will remain the same, even after parallelization.

\subsection{Indexing}

The code makes room for so-called ``halo'' cells in the fluid state.
This is a common practice in any algorithm that uses stencil-based
reconstruction to estimate variation within a domain. In this code,
there are ``hs'' halo cells on either side of each dimension, and
I pretty much hard-code hs=2.

In the Fortran code, the x- and z-dimensions are dimensioned as multi-dimensional
arrays that range from 1-hs:nx+hs. In the x-direction, 1-hs:0 belong
to the MPI task to the left, cells 1:nx belong to the current MPI
task, and nx+1:nx+hs belong to the MPI task to the right. In the z-dimension,
1-hs:0 are artificially set to mimic a solid wall boundary condition
at the bottom, and nz+1:nz+hs are the same for the top boundary.

In the C code, the fluid state array is dimensioned to size (nz+2{*}hs)
and (nx+2{*}hs) in the x- and z-directions. In the x-direction, cells
0 to hs-1 belong to the left MPI task, cells hs to nx+hs-1 belong
to the current MPI taks, and cells nx+hs to nx+2{*}hs-1 belong to
the right MPI task. The z-direction's halo cells are used to mimic
solid wall boundaries.

Since the code is periodic, the ``left'' MPI task will wrap around
the domain if you are task zero, and the ``right'' MPI task will
wrap around the domain if you are task ``nranks-1''. 

\subsection{MPI}

This code was designed to use domain decomposition, where each MPI
rank ``owns'' a certain set of cells in the x-direction and contains
two ``halo'' cells from the left- and right-had MPI tasks in the
x-direction as well. The domain is only decomposed in the $x$-direction
and not the $z$-direction.

IMPORTANT: Please be sure to set ``nranks'', ``myrank'', ``nx'',
``i\_beg'', ``left\_rank'', and ``right\_rank''. These are clearly
marked in the serial source code. You can set more variables, but
these are used elsewhere in the code (particularly in the parallel
file I/O), so they must be set.

To parallelize with MPI, there are only two places in the code that
need to be altered. The first is the initialization, a subroutine
/ function named ``init'', where you must determine the number of
ranks, you process's rank, the beginning index of your rank's first
cell in the x-direction, the number of x-direction cells your rank
will own, and the MPI rank IDs that are to your left and your right.
Because the code is periodic in the $x$-direction, your left and
right neighboring ranks will wrap around. For instance, if your are
rank zero, your left-most rank will be nranks-1.

The second place is in the routine that sets the halo values in the
$x$-direction. In this routine, you need to:
\begin{enumerate}
\item Create MPI data buffers (at the same place the other arrays are declared)
to hold the data that needs to be sent and received, allocate them
in the init() routine, and deallocate them in the finalize() routine.
\item Pack the data you need to send to your left and right MPI neighbors
\item Send the data to your left and right MPI neighbors
\item Receive the data from your left and right MPI neighbors
\item Unpack the data from your left and right neighbors and place the data
into your MPI rank's halo cells. 
\end{enumerate}
Once you complete this, the code will be fully parallelized in MPI.
Both of the places you need to add code for MPI are marked in the
serial code, and there are some extra hints in the ``set\_halo\_values\_x()''
routine as well.

\subsection{OpenMP}

For the OpenMP code, you basically need to decorate the loops with
``omp parallel do'' or ``omp parallel for'' and pay attention
to any variables you need to make ``private'' so that each thread
has its own copy. Keep in mind that OpenMP works best on ``outer''
loops rather than ``inner'' loops. Also, for sake of performance,
there are a couple of instances where it is wise to use the ``collapse''
clause because the outermost loop is not large enough to support the
number of threads most CPUs have.

\subsection{OpenACC}

The OpenACC approach will differ depending on whether you're in Fortran
or C. Just a forewarning, OpenACC is much more convenient in Fortran
when it comes to data movement because in Fortran, the compiler knows
how big your arrays are, and therefore the compiler can (and does)
create all of the data movement for you. All you have to do is optimize
the data movement after the fact.

\subsubsection{Fortran Code}

The OpenACC parallelization is a bit more involved when it comes to
performance. But, it's a good idea to just start with the kernels
themselves, since the compiler will generate all of your data statements
for you on a per-kernel basis. You need to pay attention to private
variables here as well.

Once you're getting the right answer with the kernels on the GPU,
you can look at optimizing data movement by putting in data statements.
I recommend putting data statements for the state, tend, flux, hy\_{*},
and the MPI buffers around the main time stepping loop. Then, you
need to move the data to the host before sending MPI data, back to
the device once you receive MPI data, and to the host before file
I/O.

\subsubsection{C Code}

In the C code, you'll want to put your data statements first for state,
tend, flux, hy\_{*}, and the MPI buffers. Then, as you port kernel
by kernel, be sure to transfer the requisite data back and forth with
``acc update'' statements or ``copy, copyin, and copyout'' statements
on the kernels themselves. Other than this, the approach is the same
as with the Fortran case. 

\section{Numerical Experiments}

A number of numerical experiments are in the code for you to play
around with. You can set these by changing the ``data\_spec\_int''
variable. 

\subsection{Rising Thermal}

data\_spec\_int = DATA\_SPEC\_THERMAL\\
sim\_time = 1000\\
This simulates a rising thermal in a neutral atmosphere, which will
look something like a ``mushroom'' cloud (without all of the violence).

\subsection{Colliding Thermals}

data\_spec\_int == DATA\_SPEC\_COLLISION\\
sim\_time = 600\\
This is similar to the rising thermal test case except with a cold
bubble at the model top colliding with a warm bubble at the model
bottom to produce some cool looking eddies.

\subsection{Mountain Gravity Waves}

data\_spec\_int == DATA\_SPEC\_MOUNTAIN\\
sim\_time = 1500\\
This test cases passes a horizontal wind over a faked mountain at
the model bottom in a stable atmosphere to generate a train of stationary
gravity waves across the model domain.

\subsection{Turbulence}

data\_spec\_int == DATA\_SPEC\_TURBULENCE\\
sim\_time = 1000\\
This test case creates a neutrally stratified atmosphere with randomly
initialize winds. After a few time steps, the random noise is locally
averaged out to lower values. Keep in mind that this is not classic
turbulence in any sense because it's only 2-D, and it's stratified.

\subsection{Density Current}

data\_spec\_int == DATA\_SPEC\_DENSITY\_CURRENT\\
sim\_time = 600\\
This test case creates a neutrally stratified atmosphere with a strong
cold bubble in the middle of the domain that crashes into the ground
to give the feel of a weather front (more of a downburst, really).

\section{Physics, PDEs, and Numerical Approximations}

While the numerical approximations in this code are certainly cheap
and dirty, they are probably the fastest way to get the job done at
a moderate order of accuracy. For instance, on 16 K20x GPUs, you can
perform a simulation with 5 million grid cells (3200 $\times$ 1600)
in just a few minutes.

\subsection{The 2-D Euler Equations}

This app simulates the 2-D inviscid Euler equations for stratified
fluid dynamics, which are defined as follows: 
\begin{equation}
\frac{\partial}{\partial t}\left[\begin{array}{c}
\rho\\
\rho u\\
\rho w\\
\rho\theta
\end{array}\right]+\frac{\partial}{\partial x}\left[\begin{array}{c}
\rho u\\
\rho u^{2}+p\\
\rho uw\\
\rho u\theta
\end{array}\right]+\frac{\partial}{\partial z}\left[\begin{array}{c}
\rho w\\
\rho wu\\
\rho w^{2}+p\\
\rho w\theta
\end{array}\right]=\left[\begin{array}{c}
0\\
0\\
-\rho g\\
0
\end{array}\right]\label{eq:equations_spelled_out}
\end{equation}
\[
\rho_{H}=-\frac{1}{g}\frac{\partial p}{\partial z}
\]
where $\rho$ is density, $u$, and $w$ are winds in the $x$-, and
$z$-directions, respectively, $\theta$ is potential temperature
related to temperature, $T$, by $\theta=T\left(P_{0}/P\right)^{R_{d}/c_{p}}$,
$P_{0}=10^{5}\,\text{Pa}$, $g=9.8\text{\,\ m}\,\mbox{s}^{-2}$ is
acceleration due to gravity, $p=C_{0}\left(\rho\theta\right)^{\gamma}$
is pressure, $C_{0}=R_{d}^{\gamma}p_{0}^{-R_{d}/c_{v}}$, $R_{d}=287\,\mbox{J}\,\mbox{kg}^{-1}\,\mbox{K}^{-1}$,
$\gamma=c_{p}/c_{v}$, $c_{p}=1004\,\mbox{J}\,\mbox{kg}^{-1}\,\mbox{K}^{-1}$,
$c_{v}=717\,\mbox{J}\,\mbox{kg}^{-1}\,\mbox{K}^{-1}$, and $p_{0}=10^{5}\,\mbox{Pa}$.
This can be cast in a more convenient form as:
\begin{equation}
\frac{\partial\mathbf{q}}{\partial t}+\frac{\partial\mathbf{f}}{\partial x}+\frac{\partial\mathbf{h}}{\partial z}=\mathbf{s}\label{eq:dynamics_definition}
\end{equation}

\subsection{Maintaining Hydrostatic Balance}

The flows this code simulates are relatively small perturbations off
of a ``hydrostatic'' balance, which balances gravity with a difference
in pressure:
\[
\frac{dp}{dz}=-\rho g
\]
Because small violations of this balance lead to significant noise
in the vertical momentum, it's best not to try to directly reconstruct
this balance but rather to only reconstruct the perturbations. Therefore,
hydrostasis is subtracted from (\ref{eq:equations_spelled_out}) to
give:
\begin{equation}
\frac{\partial}{\partial t}\left[\begin{array}{c}
\rho^{\prime}\\
\rho u\\
\rho w\\
\left(\rho\theta\right)^{\prime}
\end{array}\right]+\frac{\partial}{\partial x}\left[\begin{array}{c}
\rho u\\
\rho u^{2}+p\\
\rho uw\\
\rho u\theta
\end{array}\right]+\frac{\partial}{\partial z}\left[\begin{array}{c}
\rho w\\
\rho wu\\
\rho w^{2}+p^{\prime}\\
\rho w\theta
\end{array}\right]=\left[\begin{array}{c}
0\\
0\\
-\rho^{\prime}g\\
0
\end{array}\right]\label{eq:equations_spelled_out_no_hydrostasis}
\end{equation}
where a ``prime'' quantity represents that variable with the hydrostatic
background state subtracted off.

\subsection{Dimensional Splitting}

This equation is solved using dimensional splitting for simplicity
and speed. The equations are split into $x$- and $z$-direction solves
that are, respectively:
\[
x:\,\,\,\,\,\,\,\,\,\,\frac{\partial\mathbf{q}}{\partial t}+\frac{\partial\mathbf{f}}{\partial x}=\mathbf{0}
\]
\[
z:\,\,\,\,\,\,\,\,\,\,\frac{\partial\mathbf{q}}{\partial t}+\frac{\partial\mathbf{h}}{\partial x}=\mathbf{s}
\]
Each time step, the order in which the dimensions are solved is reversed,
giving second-order accuracy overall. 

\subsection{Finite-Volume Spatial Discretization\label{subsec:Finite-Volume-Spatial-Discretiza}}

A Finite-Volume discretization is used in which the PDE in a given
dimension is integrated over a cell domain, $\Omega_{i}\in\left[x_{i-1/2},x_{i+1/2}\right]$,
where $x_{i\pm1/2}=x_{i}\pm\Delta x$, $x_{i}$ is the cell center,
and $\Delta x$ is the width of the cell. The integration is the same
in the $z$-direction. Using the Gauss divergence theorem, this turns
the equation into (using the $z$-direction as an example):
\begin{equation}
\frac{\partial\overline{\mathbf{q}}_{i,k}}{\partial t}=-\frac{\mathbf{h}_{i,k+1/2}-\mathbf{h}_{i,k-1/2}}{\Delta z}+\overline{\mathbf{s}}_{i,k}\label{eq:fv_discretization}
\end{equation}
where $\overline{\mathbf{q}}_{i,k}$ and $\overline{\mathbf{s}}_{i,k}$
are the cell-average of the fluid state and source term over the cell
of index $i,k$.

To compute the update one needs the flux vector at the cell interfaces
and the cell-averaged source term. To compute the flux vector at interfaces,
fourth-order-accurate polynomial interpolation is used using the four
cell averages surrounding the cell interface in question.

\subsection{Runge-Kutta Time Integration}

The equation (\ref{eq:fv_discretization}) still has a time derivative
that needs integrating, and a simple low-storage Runge-Kutta ODE solver
is used to integrate the time derivative, and it is solved as follows:
\[
\mathbf{q}^{\star}=\mathbf{q}^{n}+\frac{\Delta t}{3}RHS\left(\mathbf{q}^{n}\right)
\]
\[
\mathbf{q}^{\star\star}=\mathbf{q}^{n}+\frac{\Delta t}{2}RHS\left(\mathbf{q}^{\star}\right)
\]
\[
\mathbf{q}^{n+1}=\mathbf{q}^{n}+\Delta tRHS\left(\mathbf{q}^{\star\star}\right)
\]

\subsection{``Hyper-viscosity''}

The centered fourth-order discretization in Section \ref{subsec:Finite-Volume-Spatial-Discretiza}
is unstable as is and requires extra dissipation to damp out small-wavelength
energy that would otherwise blow up the simulation. This damping is
accomplished with a scale-selective fourth-order so-called ``hyper''-viscosity
that is defined as:
\[
\frac{\partial\mathbf{q}}{\partial t}+\frac{\partial}{\partial x}\left(-\kappa\frac{\partial^{3}\mathbf{q}}{\partial x^{3}}\right)=\mathbf{0}
\]
and this is also solved with the Finite-Volume method just like above.
The hyperviscosity constant is defined as:
\[
\kappa=-\beta\left(\Delta x\right)^{4}2^{-4}\left(\Delta t\right)^{-1}
\]
where $\beta\in\left[0,1\right]$ is a user-defined parameter to control
the strength of the diffusion, where a higher value gives more diffusion. 
\end{document}
